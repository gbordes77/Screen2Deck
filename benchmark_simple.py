#!/usr/bin/env python3
"""Benchmark simplifiÃ© pour tester les mÃ©triques OCR"""

import sys
import os
import time
import glob
sys.path.insert(0, 'backend')

def benchmark_ocr():
    """Benchmark OCR sur les images de validation"""
    print("ğŸ¯ Benchmark OCR Screen2Deck\n")
    
    # Import EasyOCR
    try:
        import easyocr
        reader = easyocr.Reader(['en'], gpu=False)
        print("âœ… EasyOCR initialisÃ© (CPU)\n")
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return
    
    # Trouver les images
    images = glob.glob("validation_set/*.jpeg") + glob.glob("validation_set/*.jpg")
    print(f"ğŸ“¸ {len(images)} images trouvÃ©es\n")
    
    # MÃ©triques
    results = []
    total_time = 0
    
    # Traiter chaque image
    for i, img_path in enumerate(images[:5], 1):  # LimitÃ© Ã  5 pour le test
        img_name = os.path.basename(img_path)
        print(f"[{i}/{min(5, len(images))}] {img_name}")
        
        start = time.time()
        try:
            result = reader.readtext(img_path)
            elapsed = time.time() - start
            total_time += elapsed
            
            # Calculer confiance
            if result:
                mean_conf = sum(r[2] for r in result) / len(result)
            else:
                mean_conf = 0
            
            # Extraire cartes (lignes avec nombres)
            cards = []
            for bbox, text, conf in result:
                # DÃ©tection simple: commence par un chiffre
                text = text.strip()
                if text and text[0].isdigit() and conf > 0.5:
                    cards.append(text)
            
            results.append({
                'image': img_name,
                'time': elapsed,
                'zones': len(result),
                'cards': len(cards),
                'confidence': mean_conf
            })
            
            print(f"  â±ï¸ {elapsed:.2f}s | ğŸ“ {len(result)} zones | ğŸ´ {len(cards)} cartes | ğŸ“Š {mean_conf:.0%} conf")
            
        except Exception as e:
            print(f"  âŒ Erreur: {e}")
            results.append({
                'image': img_name,
                'time': 0,
                'zones': 0,
                'cards': 0,
                'confidence': 0
            })
    
    # Statistiques
    print("\n" + "="*60)
    print("ğŸ“Š RÃ‰SULTATS DU BENCHMARK")
    print("="*60)
    
    if results:
        times = [r['time'] for r in results if r['time'] > 0]
        confs = [r['confidence'] for r in results if r['confidence'] > 0]
        cards = [r['cards'] for r in results]
        
        if times:
            print(f"â±ï¸ Temps moyen: {sum(times)/len(times):.2f}s")
            print(f"â±ï¸ Temps P95: {sorted(times)[int(len(times)*0.95)]:.2f}s")
            print(f"â±ï¸ Temps max: {max(times):.2f}s")
        
        if confs:
            print(f"ğŸ“Š Confiance moyenne: {sum(confs)/len(confs):.1%}")
            print(f"ğŸ“Š Images > 62%: {sum(1 for c in confs if c >= 0.62)}/{len(confs)}")
        
        print(f"ğŸ´ Cartes moyennes/image: {sum(cards)/len(cards):.1f}")
        
        # Validation des mÃ©triques annoncÃ©es
        print("\nğŸ¯ VALIDATION DES MÃ‰TRIQUES ANNONCÃ‰ES:")
        
        # P95 < 5s ?
        if times and sorted(times)[int(len(times)*0.95)] < 5:
            print("âœ… P95 < 5s (cible atteinte)")
        else:
            print("âŒ P95 >= 5s (cible manquÃ©e)")
        
        # Accuracy (basÃ©e sur dÃ©tection de cartes)
        accuracy = sum(1 for r in results if r['cards'] >= 10) / len(results) * 100
        print(f"ğŸ“ˆ PrÃ©cision estimÃ©e: {accuracy:.1f}% (cartes dÃ©tectÃ©es >= 10)")
        
        if accuracy >= 95:
            print("âœ… Accuracy >= 95% (cible atteinte)")
        else:
            print("âš ï¸ Accuracy < 95% (mais test limitÃ©)")

if __name__ == "__main__":
    benchmark_ocr()